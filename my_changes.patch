diff --git a/.gitignore b/.gitignore
new file mode 100644
index 0000000..092b720
--- /dev/null
+++ b/.gitignore
@@ -0,0 +1,4 @@
+.DS_Store
+main1.py
+keep_alive.sh
+x.py
diff --git a/Dockerfile b/Dockerfile
new file mode 100644
index 0000000..a4d3291
--- /dev/null
+++ b/Dockerfile
@@ -0,0 +1,31 @@
+FROM python:3.9
+
+COPY --from=openjdk:8-jre-slim /usr/local/openjdk-8 /usr/local/openjdk-8
+ENV JAVA_HOME /usr/local/openjdk-8
+RUN update-alternatives --install /usr/bin/java java /usr/local/openjdk-8/bin/java 1
+
+RUN apt-get update && apt-get install -y wget vim cron
+RUN echo "alias ll='ls -lrt'" >> ~/.bashrc
+
+WORKDIR /opt/
+
+COPY keep_alive.py /opt/keep_alive.py
+COPY main.py /opt/main.py
+COPY etl_utils.py /opt/etl_utils.py
+COPY postgresql-42.5.2.jar /opt/postgresql-42.5.2.jar
+
+RUN chmod +x /opt/keep_alive.py
+RUN chmod +x /opt/main.py
+RUN chmod +x /opt/etl_utils.py
+RUN chmod 777 /opt/postgresql-42.5.2.jar
+
+RUN pip install poetry
+COPY pyproject.toml /opt/pyproject.toml
+COPY poetry.lock /opt/poetry.lock
+RUN poetry install
+
+COPY data /opt/data
+COPY tests /opt/tests
+RUN chmod -R 777 /opt/tests/
+
+CMD ["python", "/opt/keep_alive.py"]
\ No newline at end of file
diff --git a/README.md b/README.md
index 65a8396..64cd562 100644
--- a/README.md
+++ b/README.md
@@ -1,4 +1,4 @@
-# Sertis DE
+# Sertis DE take-home test
 
 ## Overview
 
diff --git a/Sertis_ETL_Solution.pptx b/Sertis_ETL_Solution.pptx
new file mode 100644
index 0000000..c4d93ec
Binary files /dev/null and b/Sertis_ETL_Solution.pptx differ
diff --git a/docker-compose.yml b/docker-compose.yml
index 96fec3d..a17e867 100644
--- a/docker-compose.yml
+++ b/docker-compose.yml
@@ -2,31 +2,62 @@ version: '3.9'
 services:
   spark:
     image: 'docker.io/bitnami/spark:3.1.2'
+    container_name: spark
     volumes:
-      - type: bind
-        source: ./data
-        target: /opt/data
+      - ./data:/opt/data
     environment:
       - SPARK_MODE=master
       - SPARK_RPC_AUTHENTICATION_ENABLED=no
       - SPARK_RPC_ENCRYPTION_ENABLED=no
       - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
       - SPARK_SSL_ENABLED=no
+      - SPARK_MASTER_MEMORY=3g
     ports:
       - '7077:7077'
       - '18080:8080'
+    networks:
+      - my_network
   spark-worker-1:
     image: docker.io/bitnami/spark:3.1.2
+    container_name: spark-worker-1
     volumes:
-      - type: bind
-        source: ./data
-        target: /opt/data
+      - ./data:/opt/data
     environment:
       - SPARK_MODE=worker
       - SPARK_MASTER_URL=spark://spark:7077
-      - SPARK_WORKER_MEMORY=1G
+      - SPARK_WORKER_MEMORY=3G
       - SPARK_WORKER_CORES=1
       - SPARK_RPC_AUTHENTICATION_ENABLED=no
       - SPARK_RPC_ENCRYPTION_ENABLED=no
       - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
       - SPARK_SSL_ENABLED=no
+    networks:
+      - my_network
+
+  postgres-db:
+    image: postgres:13
+    container_name: postgres-db
+    environment:
+      - POSTGRES_USER=postgres
+      - POSTGRES_PASSWORD=password
+      - POSTGRES_DB=warehouse
+      - POSTGRES_INITDB_ARGS=--auth-host=md5
+    ports:
+      - "5432:5432"
+    networks:
+      - my_network
+  etl:
+    build:
+      context: .
+      dockerfile: Dockerfile
+    container_name: etl
+    environment:
+      - PYTHONUNBUFFERED=1
+    volumes:
+      - ./postgresql-42.5.2.jar:/opt/postgresql-42.5.2.jar
+    networks:
+      - my_network
+
+networks:
+  my_network:
+    external: true
\ No newline at end of file
diff --git a/etl_utils.py b/etl_utils.py
new file mode 100644
index 0000000..c0fc2d6
--- /dev/null
+++ b/etl_utils.py
@@ -0,0 +1,209 @@
+import logging
+from pyspark.sql import SparkSession
+from pyspark.sql.window import Window
+from pyspark.sql.functions import col, max, sum, row_number, dense_rank, coalesce, lit, count, unix_timestamp, datediff, desc, lpad
+
+# Configure logging
+logging.basicConfig(
+  level=logging.INFO,
+  format="%(asctime)s [%(levelname)s] - %(message)s",
+  datefmt="%Y-%m-%d %H:%M:%S"
+)
+
+# PostgreSQL URL
+postgres_base_url="jdbc:postgresql://postgres-db:5432"
+
+# Method to return PostgreSQL  properties
+def get_postgres_properties():
+  database_properties = {
+    "username": "postgres",
+    "password": "password",
+    "driver": "org.postgresql.Driver"
+  }
+  return database_properties
+
+
+# Create or Get SparkSession
+def get_spark_session():
+  spark = SparkSession.builder \
+    .master('spark://spark:7077') \
+    .appName("Sertis ETL") \
+    .config("spark.jars", "/opt/postgresql-42.5.2.jar") \
+    .getOrCreate()
+  
+  spark.sparkContext.setLogLevel("WARN")
+  return spark 
+
+
+# Close SparkSession
+def close_spark_session():
+   get_spark_session().stop()
+
+
+# Read transactions (shared) as dataframe
+def read_transactions(spark, source_path):
+  # return spark.read.csv(source_path, sep='|', header=True, inferSchema=True)
+  df = spark \
+    .read \
+    .csv(source_path, sep='|', header=True)
+  
+  cols = df.columns
+  if "unitsSold" in cols:
+    df = df.withColumn(
+        "unitsSold", 
+        col("unitsSold").cast("integer")
+      )
+  
+  return df
+
+
+# Read CSV as dataframe
+def read_csv_as_dataframe(spark, source_path, header=False, delimiter=",", infer_schema=True):
+  df = spark.read.format("csv") \
+      .option("delimiter", delimiter) \
+      .option("header", header) \
+      .option("inferSchema", infer_schema) \
+      .load(source_path)
+  return df
+
+
+# Read from PostgreSQL
+def read_from_postgres(db_name, table_name):
+  # Define PostgreSQL database connection properties
+  database_properties = get_postgres_properties()
+  database_url = f"{postgres_base_url}/{db_name}"
+  
+  # Read data from PostgreSQL table into a DataFrame
+  spark = get_spark_session()
+  df = spark.read \
+    .format("jdbc") \
+    .option("url", database_url) \
+    .option("driver", "org.postgresql.Driver") \
+    .option("dbtable", f"{table_name}") \
+    .option("user", f"{database_properties.get('username')}") \
+    .option("password", f"{database_properties.get('password')}") \
+    .load()
+  
+  return df
+
+
+# Write to PostgreSQL
+def write_to_postgres(df, db_name, table_name, mode):
+  # Define PostgreSQL database connection properties
+  database_url = f"{postgres_base_url}/{db_name}"
+  database_properties = get_postgres_properties()
+  
+  # Write DataFrame to PostgreSQL table
+  df.write \
+    .mode(mode) \
+    .format("jdbc") \
+    .option("url", database_url) \
+    .option("driver", "org.postgresql.Driver") \
+    .option("dbtable", f"{table_name}") \
+    .option("user", f"{database_properties.get('username')}") \
+    .option("password", f"{database_properties.get('password')}") \
+    .save()
+
+
+# Get favourite_product
+def get_favourite_product(df):
+  # Create ranked_products DataFrame
+  # LOGIC --> total_cust_prod_sold := UnitsSold per CustId per ProductSold
+  window_spec_cust_prod = Window().partitionBy("customer_id", "productSold").orderBy()
+  ranked_products = df.withColumn(
+      "total_cust_prod_sold", 
+      sum(
+        col("unitsSold")
+      ).over(window_spec_cust_prod)
+    )
+
+  # Create cust_prod_rank DataFrame
+  # LOGIC --> rnk := Basis per ProductSold per CustId
+  window_spec_cust = Window().partitionBy("customer_id").orderBy(col("total_cust_prod_sold").desc())
+  cust_prod_rank = ranked_products.select(
+      "customer_id", 
+      "productSold",
+      "total_cust_prod_sold"
+    ) \
+    .dropDuplicates() \
+    .withColumn(
+      "rnk", 
+      dense_rank().over(window_spec_cust)
+    )
+
+  # Select the desired columns
+  result_df = cust_prod_rank.select(
+      col("customer_id"),
+      col("productSold").alias("favorite_product")
+    ).where(
+      col("rnk") == 1
+    )
+
+  return result_df
+
+# Get longest streak per custId
+def get_longest_streak(df):
+  # Sort transactions by transactionDate and assign row number
+  window_spec_cust = Window().partitionBy("customer_id").orderBy("dateDiff")
+  ranked_data = df.select(
+      col("customer_id"), 
+      col("transactionDate").cast("date")
+    ).dropDuplicates().withColumn(
+      "dateDiff", 
+      datediff(col("transactionDate"), lit("1990-01-01"))
+    ).withColumn(
+      "DateDifferenceGroup", 
+      col("dateDiff") - row_number().over(window_spec_cust)
+    )
+  
+  streak_data = ranked_data.groupBy(
+      "customer_id", 
+      "DateDifferenceGroup"
+    ).agg(
+      count(lit(1)).alias("streaks")
+    ).groupBy("customer_id").agg(
+      max(col("streaks")).alias("longest_streak")
+    )
+  return streak_data
+
+
+# Create ETL output
+def get_etl_output(favourite_product_df, longest_streak_df):
+  result_df = favourite_product_df.join(
+      longest_streak_df,
+      ['customer_id'],
+      "full_outer"
+    ).withColumn(
+      "customer_id",
+      lpad(col("customer_id"), 7, "0")
+    ).withColumnRenamed(
+      "customer_id",
+      "customer_id"
+    )
+  return result_df
+
+# Wrapper method to encapsulate ETL
+def process_etl(source_path, database_name, table_name, mode):
+  # Create a Spark Session
+  spark: SparkSession = get_spark_session()
+  
+  # Read the input file
+  # df = spark.read.csv(source_path, sep='|', header=True, inferSchema=True)
+  df = read_transactions(spark, source_path).withColumnRenamed("custId", "customer_id")
+  # df.show()
+  
+  # Write to PostgreSQL
+  write_to_postgres(df, database_name, "transaction", mode)
+  logging.info(f">>>> Raw Data [transactions] written [SaveMode: {mode}] successfully to PostgreSQL!\n")
+  
+  # Actual ETL processing
+  favourite_product_df = get_favourite_product(df)
+  longest_streak_df = get_longest_streak(df)
+  final_df = get_etl_output(favourite_product_df, longest_streak_df)
+
+  # Display the output of ETL
+  final_df.orderBy(col("longest_streak").desc()).show(50, False)
+
+  # Write output to Postgres
+  write_to_postgres(final_df, database_name, table_name, mode)
+
diff --git a/keep_alive.py b/keep_alive.py
new file mode 100644
index 0000000..530a7af
--- /dev/null
+++ b/keep_alive.py
@@ -0,0 +1,7 @@
+import time
+from datetime import datetime
+
+while True:
+    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
+    print(f"{current_time} I am alive! Going to sleep for 2 min!")
+    time.sleep(120)  # Sleep for 2 minutes (2 * 60 seconds)
\ No newline at end of file
diff --git a/main.py b/main.py
index f745cda..b1f15bb 100644
--- a/main.py
+++ b/main.py
@@ -1,31 +1,78 @@
 """Entry point for the ETL application
+# Start network
+docker network create my_network
+
+# Start Spark (master, worker) & PostgreSQL
+docker compose down --rmi all && \
+docker compose up -d spark spark-worker-1 postgres-db
+
+# RUN ETL
+docker rm --force $(docker ps -a -q --filter "name=sertisetl-etl-run") && docker image rm --force sertisetl-etl && \
+docker compose run etl poetry run python main.py --source /opt/data/transaction.csv --database warehouse --table customers --save_mode overwrite
+
+# TEST ETL
+docker rm --force $(docker ps -a -q --filter "name=sertisetl-etl-run") && docker image rm --force sertisetl-etl && \
+docker compose run etl poetry run python -m unittest discover -s /opt/tests
+
+Longest streak logic -> http://sqlfiddle.com/#!18/c5f72/6
 
 Sample usage:
+docker exec -it etl poetry run python main.py
 docker-compose run etl poetry run python main.py \
   --source /opt/data/transaction.csv \
   --database warehouse
   --table transactions
 """
-from pyspark.sql import SparkSession
-
-spark: SparkSession = SparkSession.builder.master('spark://spark:7077').getOrCreate()
-
-# TODO: implement the pipeline. Remove the lines below, they are there just to get started.
-#  Feel free to use either Spark DSL (PySpark) or Spark SQL syntax.
-#  Both examples do the same, group by transactionId and calculate the sum of `unitsSold`.
-df = spark.read.csv('path/to/my/file/transaction.csv', sep='|', header=True, inferSchema=True)
-
-#################################################################
-# Spark DSL example
-df.groupBy('transactionId').sum('unitsSold').show()
-#################################################################
-
-#################################################################
-# Spark SQL example
-df.createOrReplaceTempView('transaction')
-spark.sql("""
-select transactionId, sum(unitsSold)
-from transaction
-group by transactionId
-""").show()
-#################################################################
+
+import argparse
+import logging
+from etl_utils import close_spark_session, process_etl
+
+def main():
+  # Configure logging
+  logging.basicConfig(
+      level=logging.INFO,  # Set the desired logging level
+      format="%(asctime)s [%(levelname)s] - %(message)s",
+      datefmt="%Y-%m-%d %H:%M:%S"
+  )
+  
+  # Validate arguments
+  parser = argparse.ArgumentParser(description="Process data.")
+  parser.add_argument("--source", required=True, help="Path to the source file")
+  parser.add_argument("--database", required=True, help="Database name for inserting data")
+  parser.add_argument("--table", required=True, help="Table name for inserting data")
+  parser.add_argument("--save_mode", required=False, choices=["append", "overwrite"], help="Save mode choices: append|overwrite")
+
+  save_mode = "append"
+  default_save_mode = True
+  
+  args = parser.parse_args()
+
+  source_path = args.source
+  database_name = args.database
+  table_name = args.table
+  
+  if args.save_mode is not None:
+      default_save_mode = False
+      save_mode = args.save_mode
+  msg = "[default]" if default_save_mode else "[user provided]"
+
+  # Your code to process data goes here
+  logging.info("")
+  logging.info(f">>>> Processing data from: {source_path}")
+  logging.info(f">>>> Database: {database_name}")
+  logging.info(f">>>> Table: {table_name}")
+  logging.info(f">>>> Save mode: {save_mode} {msg}\n")
+  
+  try: 
+    # Process ETL
+    process_etl(source_path, database_name, table_name, save_mode)
+    logging.info(">>>> ETL SUCCESSFUL!\n")
+
+  finally:
+    # Close SparkSession if still open
+    close_spark_session()
+
+
+if __name__ == "__main__":
+  main()
\ No newline at end of file
diff --git a/poetry.lock b/poetry.lock
index 3a99708..ac94189 100644
--- a/poetry.lock
+++ b/poetry.lock
@@ -21,6 +21,7 @@ py4j = "0.10.9"
 ml = ["numpy (>=1.7)"]
 mllib = ["numpy (>=1.7)"]
 sql = ["pandas (>=0.23.2)", "pyarrow (>=1.0.0)"]
+postgresql = ["psycopg2"]
 
 [metadata]
 lock-version = "1.1"
diff --git a/postgresql-42.5.2.jar b/postgresql-42.5.2.jar
new file mode 100755
index 0000000..368fd77
Binary files /dev/null and b/postgresql-42.5.2.jar differ
diff --git a/sql_scratch_notes.sql b/sql_scratch_notes.sql
new file mode 100644
index 0000000..db8e62d
--- /dev/null
+++ b/sql_scratch_notes.sql
@@ -0,0 +1,68 @@
+-- Longest streak for Customer_Id
+-- ==============================
+WITH transaction1 AS (
+  SELECT DISTINCT
+    "custId",
+    CAST("transactionDate" AS DATE) AS "transactionDate",
+    CAST('1990-01-01' AS DATE) AS "referenceDate",
+    CAST("transactionDate" AS DATE) - CAST('1990-01-01' AS DATE) AS "DateDifference"
+  FROM 
+    "transaction"
+),
+date_diff_group AS (
+  SELECT
+    *,
+    "DateDifference" - ROW_NUMBER() OVER(PARTITION BY "custId" ORDER BY "DateDifference") AS "DateDifferenceGroup"
+  FROM "transaction1"
+  ORDER BY "custId"
+),
+days_streaks AS (
+  SELECT 
+    "custId",
+    "DateDifferenceGroup",
+    COUNT(1) AS "streak"
+  FROM "date_diff_group"
+  GROUP BY 
+    "custId",
+    "DateDifferenceGroup"
+),
+fun_analysis AS (
+  SELECT 
+    "custId",
+    MAX("streak") AS "longest_streak"
+  FROM
+    "days_streaks"
+  GROUP BY
+    "custId"
+)
+SELECT * FROM "fun_analysis" 
+WHERE "custId" = 23938; 
+
+
+-- Favorite Product for Customer_Id
+-- ================================
+WITH ranked_products AS (
+  SELECT
+    "trans_id",
+    "custId",
+    "trans_date",
+    "productSold",
+    "unitsSold",
+    SUM("unitsSold") OVER (PARTITION BY "custId", "productSold") AS "total_cust_prod_sold"
+  FROM "transactions"
+),
+cust_prod_rank AS (
+SELECT 
+  "custId" AS "customer_id",
+  "productSold",
+  "total_cust_prod_sold",
+  DENSE_RANK() OVER(PARTITION BY "custId" ORDER BY "total_cust_prod_sold" DESC) as "rnk"
+FROM 
+  "ranked_products"
+)
+SELECT
+  "custId", "productSold" AS "favourite_product"
+FROM 
+  "cust_prod_rank" 
+WHERE 
+  "rnk" = 1;
diff --git a/tests/__init__.py b/tests/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/tests/test_etl.py b/tests/test_etl.py
new file mode 100644
index 0000000..02e356c
--- /dev/null
+++ b/tests/test_etl.py
@@ -0,0 +1,153 @@
+import unittest
+import logging
+from pyspark.sql import SparkSession
+
+from etl_utils import *
+
+# Configure logging
+logging.basicConfig(
+  level=logging.INFO,
+  format="%(asctime)s [%(levelname)s] - %(message)s",
+  datefmt="%Y-%m-%d %H:%M:%S"
+)
+
+class TestETL(unittest.TestCase):
+  def setUp(self):
+    self.spark = get_spark_session()
+
+  def tearDown(self):
+    self.spark.stop()
+
+  def read_csv_data(self):
+    input_path = "/opt/data/transaction.csv"
+    df = read_transactions(self.spark, input_path).withColumnRenamed("custId", "customer_id")
+    df.cache()
+    return df
+
+  # Unit test favorite product 
+  def test_favorite_product_and_longest_streak(self):
+    # Expected favorite products for customers
+    fav_prod_dict = {
+      "0024003" : "SUPA101",
+      "0023824" : "PURA250",
+      "0024212" : "PURA100",
+      "0023510" : "PURA100"
+    }
+    # Expected longest streak for customers
+    streak_dict = {
+      "0024003" : 3,
+      "0023824" : 3,
+      "0024212" : 2,
+      "0023510" : 1
+    }
+
+    # Read CSV input and compute ETL output
+    df = self.read_csv_data().filter(
+        col("customer_id").isin(list(fav_prod_dict.keys()))
+      )
+    etl_output = get_etl_output(
+        get_favourite_product(df),
+        get_longest_streak(df)
+      )
+    data = etl_output.collect()
+    result_dict = {row["customer_id"]: [row["favorite_product"], row["longest_streak"]] for row in data}
+    
+    assert len(data) == len(fav_prod_dict.keys()), f"ETL output has missing customer_ids!"
+
+    for cust_id in fav_prod_dict.keys():
+      prod_id = result_dict.get(cust_id)[0]
+      streak = result_dict.get(cust_id)[1]
+
+      with self.subTest(cust_id):
+        assert prod_id == fav_prod_dict.get(cust_id), f"Favorite product  test failed for [{cust_id}]. Expected [{fav_prod_dict.get(cust_id)}], actual: [{prod_id}]"
+        logging.info(f">>>> Favorite Product [{cust_id}, {prod_id}] is VALID!")
+
+      with self.subTest(cust_id):
+        assert streak == streak_dict.get(cust_id), f"Favorite product  test failed for [{cust_id}]. Expected [{expected_longest_streak}], actual: [{streak}]"
+        logging.info(f">>>> Streak [{cust_id}, {streak}] is VALID!")
+
+  # Unit test favorite product 
+  @unittest.skip("Skipping this test for now as we have test_favorite_product_and_longest_streak()")
+  def test_favorite_product(self):
+    # Expected favorite products for customers
+    fav_prod_dict = {
+      "0024003" : "SUPA101",
+      "0023824" : "PURA250",
+      "0023855" : "PURA100",
+      "0024221" : "PURA100"
+    }
+
+    # Compute favorite products
+    df = self.read_csv_data().filter(col("customer_id").isin(list(fav_prod_dict.keys())))
+    fav_prod_df = get_favourite_product(df)
+    data = fav_prod_df.collect()
+    result_dict = {row["customer_id"]: row["favorite_product"] for row in data}
+    
+    # Tetsing computed favorite products against expected
+    for cust_id, expected_fav_product in fav_prod_dict.items():
+      with self.subTest(cust_id=cust_id):
+        self.assertEqual(
+          result_dict.get(cust_id), 
+          expected_fav_product, 
+          f"Longest streak test failed forr customer_id: [{cust_id}]. Expected: [{expected_fav_product}], actual: [{result_dict.get(cust_id)}]."
+        )
+
+  # Unit test longest streak
+  @unittest.skip("Skipping this test for now as we have test_favorite_product_and_longest_streak()")
+  def test_longest_streak(self):
+    # Expected longest streak for customers
+    streak_dict = {
+      "0024003" : 3,
+      "0023824" : 3,
+      "0024024" : 2,
+      "0023658" : 2,
+      "0023624" : 1
+    }
+
+    # Compute longest streak
+    df = self.read_csv_data().filter(col("customer_id").isin(list(streak_dict.keys())))
+    longest_streak_df = get_longest_streak(df)
+    data = longest_streak_df.collect()
+    result_dict = {row["customer_id"]: row["longest_streak"] for row in data}
+
+    # Tetsing computed favorite products against expected
+    for cust_id, expected_longest_streak in streak_dict.items():
+      with self.subTest(cust_id=cust_id):
+        self.assertEqual(
+          result_dict.get(cust_id), 
+          expected_longest_streak, 
+          f"Longest streak test failed forr customer_id: [{cust_id}]. Expected: [{expected_longest_streak}], actual: [{result_dict.get(cust_id)}]."
+        )
+
+  def test_check_specific_customer(self):
+    # Specifications
+    cust_id = "0023938"
+    expected_fav_product = "PURA250"
+    expected_longest_streak = 2
+
+    # Read CSV input and compute ETL output
+    df = self.read_csv_data().filter(
+        col("customer_id") == cust_id
+      )
+    etl_output = get_etl_output(
+        get_favourite_product(df),
+        get_longest_streak(df)
+      ).select(
+        "favorite_product", 
+        "longest_streak"
+      ).limit(1).collect()
+    
+    favourite_product = etl_output[0][0]
+    longest_streak = etl_output[0][1]
+
+    with self.subTest():
+      assert favourite_product == expected_fav_product, f"Favorite product  test failed for [{cust_id}]. Expected [{expected_fav_product}], actual: [{favourite_product}]"
+      logging.info(f">>>> Customer ID [{cust_id}] : Favorite Product is VALID!")
+
+    with self.subTest():
+      assert longest_streak == expected_longest_streak, f"Favorite product  test failed for [{cust_id}]. Expected [{expected_longest_streak}], actual: [{longest_streak}]"
+      logging.info(f">>>> Customer ID [{cust_id}] : Streak is VALID!")
+    
+
+if __name__ == "__main__":
+  unittest.main()
